{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57f2ef9f",
   "metadata": {},
   "source": [
    "# Abstract Syntax Tree (AST) Preprocessing for Machine Learning Models\n",
    "Converting source code into a format suitable for machine learning models requires several transformation steps. This document outlines the comprehensive preprocessing pipeline that transforms raw code into vectorized representations that machine learning models can process effectively.\n",
    "\n",
    "## Key Components\n",
    "1. AST Flattening\n",
    "   1. The `flatten_ast` function captures both node types and structural information\n",
    "   2. Tracks parent-child relationships via the path parameter\n",
    "   3. Extracts values from nodes when available\n",
    "2. Tokenization Strategy\n",
    "   1. Creates three types of tokens:\n",
    "      1. Node type tokens (`TYPE_X`)\n",
    "      2. Structural relationship tokens (`PARENT_X_TO_Y`)\n",
    "      3. Value tokens for identifiers and literals (`VAL_X` or `LIT_type`)\n",
    "   2. This preserves both syntactic structure and semantic information\n",
    "3. Vectorization Options\n",
    "   1. Two complementary approaches:\n",
    "      1. Sequence-based: Preserves order of AST nodes using vocabulary mapping\n",
    "      2. Bag-of-nodes: Creates frequency-based vector representations, useful for classification tasks\n",
    "4. Vocabulary Management:\n",
    "   1. Creates a vocabulary with frequency thresholding\n",
    "   2. Includes special tokens for padding and unknown tokens\n",
    "   3. Enables consistent encoding across different code samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "150b24da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import javalang\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import pickle\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a1483570",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_code_file(file_path):\n",
    "    \"\"\"Read code from a file.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            return file.read()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file {file_path}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d7e20859",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ast(code):\n",
    "    \"\"\"\n",
    "    Creates an Abstract Syntax Tree (AST) from the given code.\n",
    "\n",
    "    Args:\n",
    "        code (str): The code to parse.\n",
    "\n",
    "    Returns:\n",
    "        javalang.tree.CompilationUnit: The AST of the code.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        tree = javalang.parse.parse(code)\n",
    "        return tree\n",
    "    except javalang.parser.JavaSyntaxError as e:\n",
    "        print(f\"Syntax error in code: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "68c45db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_ast(node, token_types=None, path=\"\", result=None):\n",
    "    \"\"\"\n",
    "    Flattens an AST into a sequence of node types with their paths.\n",
    "\n",
    "    Args:\n",
    "        node: The current AST node\n",
    "        token_types: Dictionary to track seen token types\n",
    "        path: Current path in the AST\n",
    "        result: List to collect flattened nodes\n",
    "\n",
    "    Returns:\n",
    "        List of tuples: (node_type, path, value)\n",
    "    \"\"\"\n",
    "    if result is None:\n",
    "        result = []\n",
    "\n",
    "    if token_types is None:\n",
    "        token_types = {}\n",
    "\n",
    "    # Skip if node is None\n",
    "    if node is None:\n",
    "        return result\n",
    "\n",
    "    # Process the current node\n",
    "    node_type = node.__class__.__name__\n",
    "\n",
    "    # Track token types\n",
    "    if node_type not in token_types:\n",
    "        token_types[node_type] = len(token_types)\n",
    "\n",
    "    # Extract value if available (for identifiers, literals, etc.)\n",
    "    value = None\n",
    "    if hasattr(node, \"name\"):\n",
    "        value = node.name\n",
    "    elif hasattr(node, \"value\"):\n",
    "        value = node.value\n",
    "\n",
    "    # Add the current node to the result\n",
    "    result.append((node_type, path, value))\n",
    "\n",
    "    # Recursively process children\n",
    "    if hasattr(node, \"children\"):\n",
    "        for i, child in enumerate(node.children):\n",
    "            child_path = f\"{path}/{node_type}_{i}\"\n",
    "            if isinstance(child, list):\n",
    "                for j, item in enumerate(child):\n",
    "                    if hasattr(item, \"__class__\"):\n",
    "                        flatten_ast(item, token_types, f\"{child_path}_{j}\", result)\n",
    "            elif hasattr(child, \"__class__\"):\n",
    "                flatten_ast(child, token_types, child_path, result)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2b1bafca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_ast(flattened_ast):\n",
    "    \"\"\"\n",
    "    Convert a flattened AST to a sequence of tokens.\n",
    "\n",
    "    Args:\n",
    "        flattened_ast: List of (node_type, path, value) tuples\n",
    "\n",
    "    Returns:\n",
    "        List of string tokens\n",
    "    \"\"\"\n",
    "    tokens = []\n",
    "\n",
    "    for node_type, path, value in flattened_ast:\n",
    "        # Add node type as token\n",
    "        tokens.append(f\"TYPE_{node_type}\")\n",
    "\n",
    "        # Add simplified path to capture structural information\n",
    "        path_components = path.split(\"/\")\n",
    "        if len(path_components) > 1:\n",
    "            parent = path_components[-2].split(\"_\")[0]\n",
    "            tokens.append(f\"PARENT_{parent}_TO_{node_type}\")\n",
    "\n",
    "        # Add value if present (for identifiers, literals, etc.)\n",
    "        if value is not None:\n",
    "            # Handle different types of values\n",
    "            if isinstance(value, str):\n",
    "                # For identifiers, method names, etc.\n",
    "                tokens.append(f\"VAL_{value}\")\n",
    "            elif isinstance(value, (int, float, bool)):\n",
    "                # For numeric literals\n",
    "                tokens.append(f\"LIT_{type(value).__name__}\")\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ab973f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocabulary(all_tokens, min_freq=2):\n",
    "    \"\"\"\n",
    "    Create a vocabulary from all tokens.\n",
    "\n",
    "    Args:\n",
    "        all_tokens: List of token lists\n",
    "        min_freq: Minimum frequency for a token to be included\n",
    "\n",
    "    Returns:\n",
    "        Dictionary mapping tokens to indices\n",
    "    \"\"\"\n",
    "    # Count token frequencies\n",
    "    token_counts = defaultdict(int)\n",
    "    for tokens in all_tokens:\n",
    "        for token in tokens:\n",
    "            token_counts[token] += 1\n",
    "\n",
    "    # Create vocabulary with tokens that meet minimum frequency\n",
    "    vocabulary = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "    for token, count in token_counts.items():\n",
    "        if count >= min_freq:\n",
    "            vocabulary[token] = len(vocabulary)\n",
    "\n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6ad57b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_tokens(tokens, vocabulary, max_length=None):\n",
    "    \"\"\"\n",
    "    Convert token list to vector using vocabulary.\n",
    "\n",
    "    Args:\n",
    "        tokens: List of tokens\n",
    "        vocabulary: Token to index mapping\n",
    "        max_length: Maximum length of vector (pad/truncate)\n",
    "\n",
    "    Returns:\n",
    "        Numpy array of token indices\n",
    "    \"\"\"\n",
    "    if max_length is None:\n",
    "        max_length = len(tokens)\n",
    "\n",
    "    # Convert tokens to indices\n",
    "    vector = []\n",
    "    for i, token in enumerate(tokens[:max_length]):\n",
    "        if token in vocabulary:\n",
    "            vector.append(vocabulary[token])\n",
    "        else:\n",
    "            vector.append(vocabulary[\"<UNK>\"])\n",
    "\n",
    "    # Pad if necessary\n",
    "    if len(vector) < max_length:\n",
    "        vector.extend([vocabulary[\"<PAD>\"]] * (max_length - len(vector)))\n",
    "\n",
    "    return np.array(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "09534988",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(vocabulary, embedding_dim=100):\n",
    "    \"\"\"Create initial random embeddings for tokens.\"\"\"\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    vocab_size = len(vocabulary)\n",
    "    embeddings = np.random.normal(0, 1, (vocab_size, embedding_dim))\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37deabac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(dataset_path, embedding_dim=100):\n",
    "    \"\"\"\n",
    "    Process all files in the dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset_path: Path to the dataset directory\n",
    "        embedding_dim: Dimension for token embeddings\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with processed data\n",
    "    \"\"\"\n",
    "    token_types = {}\n",
    "    all_flattened_asts = []\n",
    "    all_tokenized_asts = []\n",
    "    file_paths = []\n",
    "\n",
    "    for file in os.listdir(dataset_path):\n",
    "        file_path = os.path.join(dataset_path, file)\n",
    "        code = read_code_file(file_path)\n",
    "\n",
    "        if code:\n",
    "            tree = create_ast(code)\n",
    "            if tree:\n",
    "                flattened = flatten_ast(tree, token_types)\n",
    "                all_flattened_asts.append(flattened)\n",
    "\n",
    "                # Tokenize AST\n",
    "                tokens = tokenize_ast(flattened)\n",
    "                all_tokenized_asts.append(tokens)\n",
    "\n",
    "                file_paths.append(file_path)\n",
    "            else:\n",
    "                print(f\"Failed to create AST for {file}.\")\n",
    "        else:\n",
    "            print(f\"Failed to read code from {file}.\")\n",
    "\n",
    "    # Create vocabulary from all tokens\n",
    "    vocabulary = create_vocabulary(all_tokenized_asts)\n",
    "\n",
    "    # Create embeddings for vocabulary\n",
    "    embeddings = create_embeddings(vocabulary, embedding_dim)\n",
    "\n",
    "    # Get max sequence length for padding\n",
    "    max_length = max(len(tokens) for tokens in all_tokenized_asts)\n",
    "\n",
    "    # Vectorize all token sequences\n",
    "    vectorized_sequences = [\n",
    "        vectorize_tokens(tokens, vocabulary, max_length)\n",
    "        for tokens in all_tokenized_asts\n",
    "    ]\n",
    "\n",
    "    return {\n",
    "        \"token_types\": token_types,\n",
    "        \"vocabulary\": vocabulary,\n",
    "        \"embeddings\": embeddings,\n",
    "        \"flattened_asts\": all_flattened_asts,\n",
    "        \"tokenized_asts\": all_tokenized_asts,\n",
    "        \"sequence_vectors\": vectorized_sequences,\n",
    "        \"file_paths\": file_paths,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7a9c2c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_deep_learning(processed_data, batch_size=32):\n",
    "    \"\"\"\n",
    "    Prepare data for deep learning models.\n",
    "\n",
    "    Args:\n",
    "        processed_data: Dictionary with processed data\n",
    "        batch_size: Batch size for training\n",
    "\n",
    "    Returns:\n",
    "        Dataset ready for deep learning training\n",
    "    \"\"\"\n",
    "    # Convert to appropriate format\n",
    "    sequence_vectors = np.array(processed_data[\"sequence_vectors\"])\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(sequence_vectors)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    return dataset, processed_data[\"embeddings\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "586148ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_processed_data(data, output_file):\n",
    "    \"\"\"Save processed data to disk.\"\"\"\n",
    "    with open(output_file, \"wb\") as f:\n",
    "        pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b708e2b0",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c0723090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30306\n",
      "Processed 975 files\n",
      "Found 57 unique token types\n",
      "Vocabulary size: 2666\n",
      "\n",
      "Sample of first tokenized AST:\n",
      "['TYPE_CompilationUnit', 'TYPE_Import', 'PARENT__TO_Import', 'TYPE_str', 'PARENT_CompilationUnit_TO_str', 'TYPE_bool', 'PARENT_CompilationUnit_TO_bool', 'TYPE_bool', 'PARENT_CompilationUnit_TO_bool', 'TYPE_Import', 'PARENT__TO_Import', 'TYPE_str', 'PARENT_CompilationUnit_TO_str', 'TYPE_bool', 'PARENT_CompilationUnit_TO_bool', 'TYPE_bool', 'PARENT_CompilationUnit_TO_bool', 'TYPE_Import', 'PARENT__TO_Import', 'TYPE_str']\n"
     ]
    }
   ],
   "source": [
    "dataset_path = \"../../datasets/conplag_preprocessed\"\n",
    "processed_data = process_dataset(dataset_path)\n",
    "\n",
    "print(f\"Processed {len(processed_data['file_paths'])} files\")\n",
    "print(f\"Found {len(processed_data['token_types'])} unique token types\")\n",
    "print(f\"Vocabulary size: {len(processed_data['vocabulary'])}\")\n",
    "\n",
    "# Save the processed data\n",
    "save_processed_data(processed_data, \"ast_processed_data.pkl\")\n",
    "\n",
    "# Example: accessing the first tokenized AST\n",
    "if processed_data[\"tokenized_asts\"]:\n",
    "    print(\"\\nSample of first tokenized AST:\")\n",
    "    print(processed_data[\"tokenized_asts\"][0][:20])  # First 20 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "12ad8417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset prepared with 975 samples\n",
      "Embedding matrix shape: (2666, 100)\n",
      "Embedding example: [ 0.49671415 -0.1382643   0.64768854  1.52302986 -0.23415337 -0.23413696\n",
      "  1.57921282  0.76743473 -0.46947439  0.54256004 -0.46341769 -0.46572975\n",
      "  0.24196227 -1.91328024 -1.72491783 -0.56228753 -1.01283112  0.31424733\n",
      " -0.90802408 -1.4123037 ]\n"
     ]
    }
   ],
   "source": [
    "# For deep learning workflows\n",
    "dataset, embeddings = prepare_for_deep_learning(processed_data)\n",
    "\n",
    "print(f\"Dataset prepared with {len(processed_data['file_paths'])} samples\")\n",
    "print(f\"Embedding matrix shape: {embeddings.shape}\")\n",
    "print(\n",
    "    f\"Embedding example: {embeddings[0][:20]}\"\n",
    ")  # First 5 values of the first embedding"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
