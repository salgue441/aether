{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3602973",
   "metadata": {},
   "source": [
    "## Ã†ther: Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b7b193",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc425c32",
   "metadata": {},
   "source": [
    "## Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb76b054",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlagiarismDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset implementation for code plagiarism detection\n",
    "\n",
    "    This class handles pairs of code files, reading them from specific\n",
    "    directories, tokenizing them together, and preparing them for model input.\n",
    "\n",
    "    Attributes:\n",
    "        file_pairs (list): List of tuples containing pairs of files IDS (id1, id2)\n",
    "        labels (list): List of binary labels (0 for no plagiarism, 1 for plagiarism)\n",
    "        base_dir (Path): Base directory containing the code files\n",
    "        tokenizer: HuggingFace tokenizer for encoding code pairs\n",
    "        max_length (int): Maximum sequence length for tokenization\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, file_pairs, labels, base_dir, tokenizer, max_length=512):\n",
    "        \"\"\"\n",
    "        Initialize the plagiarism dataset.\n",
    "\n",
    "        Args:\n",
    "            file_pairs (list): List of tuples containing pairs of file IDs to compare\n",
    "            labels (list): Corresponding binary labels (0/1) indicating plagiarism\n",
    "            base_dir (str or Path): Root directory containing the code files\n",
    "            tokenizer: HuggingFace tokenizer for encoding the code\n",
    "            max_length (int, optional): Maximum sequence length for tokenization. Defaults to 512.\n",
    "        \"\"\"\n",
    "\n",
    "        self.file_pairs = file_pairs\n",
    "        self.labels = labels\n",
    "        self.base_dir = Path(base_dir)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Get the number of code pairs in the dataset.\n",
    "\n",
    "        Returns:\n",
    "            int: Number of code pairs\n",
    "        \"\"\"\n",
    "\n",
    "        return len(self.file_pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get a code pair and its label by index.\n",
    "\n",
    "        This method:\n",
    "        1. Retrieves file IDs and label for the specified index\n",
    "        2. Reads code content from both files\n",
    "        3. Tokenizes both code samples together\n",
    "        4. Returns a dictionary with tokenized inputs and label\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of the code pair to retrieve\n",
    "\n",
    "        Returns:\n",
    "            dict: Dictionary containing:\n",
    "                - 'input_ids': Tokenized input IDs\n",
    "                - 'attention_mask': Attention mask for the tokenized input\n",
    "                - 'label': Tensor containing the plagiarism label\n",
    "        \"\"\"\n",
    "\n",
    "        id1, id2 = self.file_pairs[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        code1 = self._read_file(id1)\n",
    "        code2 = self._read_file(id2)\n",
    "\n",
    "        encoded = self.tokenizer(\n",
    "            code1,\n",
    "            code2,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": encoded[\"input_ids\"].squeeze(),\n",
    "            \"attention_mask\": encoded[\"attention_mask\"].squeeze(),\n",
    "            \"label\": torch.tensor(label, dtype=torch.long),\n",
    "        }\n",
    "\n",
    "    def _read_file(self, file_id):\n",
    "        \"\"\"\n",
    "        Read a code file from either the plagiarism or non-plagiarism directory.\n",
    "\n",
    "        This method tries multiple possible file paths:\n",
    "        1. In the 'plagio' directory with and without .txt extension\n",
    "        2. In the 'no plagio' directory with and without .txt extension\n",
    "\n",
    "        It handles potential encoding issues by trying both UTF-8 and Latin-1 encodings.\n",
    "\n",
    "        Args:\n",
    "            file_id: ID of the file to read\n",
    "\n",
    "        Returns:\n",
    "            str: Content of the code file or empty string if file not found\n",
    "        \"\"\"\n",
    "\n",
    "        plagio_path = self.base_dir / \"plagio\" / f\"{file_id}\"\n",
    "        no_plagio_path = self.base_dir / \"no_plagio\" / f\"{file_id}\"\n",
    "\n",
    "        paths = [\n",
    "            plagio_path,\n",
    "            no_plagio_path,\n",
    "            Path(str(plagio_path) + \".txt\"),\n",
    "            Path(str(no_plagio_path) + \".txt\"),\n",
    "        ]\n",
    "\n",
    "        for path in paths:\n",
    "            if path.exists():\n",
    "                try:\n",
    "                    return path.read_text(encoding=\"utf-8\")\n",
    "\n",
    "                except UnicodeDecodeError:\n",
    "                    return path.read_text(encoding=\"latin-1\")\n",
    "\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6899fe4c",
   "metadata": {},
   "source": [
    "## Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f30665",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlagiarismModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural network model for code plagiarism detection based on CodeBERT embeddings.\n",
    "\n",
    "    This model utilizes CodeBERT to extract contextual embeddings from pairs of\n",
    "    code samples, then passes the CLS token representation through a classifier\n",
    "    network to predict whether the code pair exhibits plagiarism.\n",
    "\n",
    "    Attributes:\n",
    "        codebert: Pretrained CodeBERT model for extracting code embeddings\n",
    "        classifier: Sequential neural network for binary classification\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_name):\n",
    "        \"\"\"\n",
    "        Initialize the plagiarism detection model.\n",
    "\n",
    "        Args:\n",
    "            model_name (str): Name or path of the pretrained CodeBERT model\n",
    "                              (e.g., \"microsoft/codebert-base\")\n",
    "\n",
    "        Note:\n",
    "            The model assumes CodeBERT's hidden size is 768 dimensions. If using\n",
    "            a different pretrained model, this value may need adjustment.\n",
    "        \"\"\"\n",
    "\n",
    "        super(PlagiarismModel, self).__init__()\n",
    "\n",
    "        self.codebert = AutoModel.from_pretrained(model_name)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(768, 256),  # Reduce from CodeBERT's 768 dimensions to 256\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(256, 2),\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "\n",
    "        The model processes tokenized code pairs through CodeBERT, extracts the\n",
    "        CLS token representation (which encodes the relationship between the two\n",
    "        code samples), and passes it through the classifier to get plagiarism\n",
    "        prediction logits.\n",
    "\n",
    "        Args:\n",
    "            input_ids (torch.Tensor): Tokenized input IDs of code pairs\n",
    "            attention_mask (torch.Tensor): Attention mask for the input\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Logits for binary classification (no plagiarism, plagiarism)\n",
    "        \"\"\"\n",
    "\n",
    "        outputs = self.codebert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]\n",
    "\n",
    "        return self.classifier(cls_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a34eaea",
   "metadata": {},
   "source": [
    "## Detection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cd3687",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlagiarismDetection:\n",
    "    \"\"\"\n",
    "    Main class for code plagiarism detection that orchestrates the entire workflow.\n",
    "\n",
    "    This class handles the complete plagiarism detection pipeline including:\n",
    "    - Model initialization and configuration\n",
    "    - Data loading and preprocessing\n",
    "    - Model training and evaluation\n",
    "    - Model saving and loading\n",
    "\n",
    "    Attributes:\n",
    "        model_name (str): Name or path of the CodeBERT model\n",
    "        tokenizer: HuggingFace tokenizer for encoding code pairs\n",
    "        model: The neural network model for plagiarism detection\n",
    "        device: PyTorch device for computation (CPU or GPU)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_name=\"microsoft/codebert-base\"):\n",
    "        \"\"\"\n",
    "        Initialize the plagiarism detection system.\n",
    "\n",
    "        Args:\n",
    "            model_name (str, optional): Pretrained model identifier to use.\n",
    "                                       Defaults to \"microsoft/codebert-base\".\n",
    "        \"\"\"\n",
    "\n",
    "        self.model_name = model_name\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = PlagiarismModel(model_name)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def load_data(self, csv_path, data_dir):\n",
    "        \"\"\"\n",
    "        Load and prepare data for training and testing.\n",
    "\n",
    "        Reads the CSV file containing code pair IDs and plagiarism labels,\n",
    "        splits the data into training and test sets, and creates DataLoader\n",
    "        instances for efficient batch processing.\n",
    "\n",
    "        Args:\n",
    "            csv_path (str): Path to the CSV file with columns 'id1', 'id2', 'plagio'\n",
    "            data_dir (str): Directory containing code files\n",
    "\n",
    "        Returns:\n",
    "            tuple: (train_loader, test_loader) - DataLoader instances for training and testing\n",
    "        \"\"\"\n",
    "\n",
    "        df = pd.read_csv(csv_path)\n",
    "        file_pairs = list(zip(df[\"id1\"], df[\"id2\"]))\n",
    "        labels = df[\"plagio\"].tolist()\n",
    "\n",
    "        train_pairs, test_pairs, train_labels, test_labels = train_test_split(\n",
    "            file_pairs, labels, test_size=0.2, random_state=42, stratify=labels\n",
    "        )\n",
    "\n",
    "        train_dataset = PlagiarismDataset(\n",
    "            train_pairs, train_labels, data_dir, self.tokenizer\n",
    "        )\n",
    "        test_dataset = PlagiarismDataset(\n",
    "            test_pairs, test_labels, data_dir, self.tokenizer\n",
    "        )\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "        return train_loader, test_loader\n",
    "\n",
    "    def train(self, train_loader, epochs=3):\n",
    "        \"\"\"\n",
    "        Train the plagiarism detection model.\n",
    "\n",
    "        Performs training for the specified number of epochs, tracking\n",
    "        loss and accuracy metrics throughout the process.\n",
    "\n",
    "        Args:\n",
    "            train_loader (DataLoader): DataLoader containing training data\n",
    "            epochs (int, optional): Number of training epochs. Defaults to 3.\n",
    "        \"\"\"\n",
    "\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=2e-5)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        self.model.train()\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "\n",
    "            for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "                input_ids = batch[\"input_ids\"].to(self.device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(self.device)\n",
    "                labels = batch[\"label\"].to(self.device)\n",
    "\n",
    "                # Forward pass\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(input_ids, attention_mask)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # Track metrics\n",
    "                total_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "            avg_loss = total_loss / len(train_loader)\n",
    "            accuracy = 100 * correct / total\n",
    "            print(f\"Epoch {epoch+1}: Loss={avg_loss:.4f}, Accuracy={accuracy:.2f}%\")\n",
    "\n",
    "    def evaluate(self, test_loader):\n",
    "        \"\"\"\n",
    "        Evaluate the model on test data.\n",
    "\n",
    "        Sets the model to evaluation mode and calculates accuracy on the test set.\n",
    "\n",
    "        Args:\n",
    "            test_loader (DataLoader): DataLoader containing test data\n",
    "\n",
    "        Returns:\n",
    "            float: Accuracy percentage on test data\n",
    "        \"\"\"\n",
    "\n",
    "        self.model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "                input_ids = batch[\"input_ids\"].to(self.device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(self.device)\n",
    "                labels = batch[\"label\"].to(self.device)\n",
    "\n",
    "                outputs = self.model(input_ids, attention_mask)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        accuracy = 100 * correct / total\n",
    "        print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "        return accuracy\n",
    "\n",
    "    def save_model(self, path):\n",
    "        \"\"\"\n",
    "        Save the trained model to a file.\n",
    "\n",
    "        Args:\n",
    "            path (str): Path where the model should be saved\n",
    "        \"\"\"\n",
    "\n",
    "        torch.save(self.model.state_dict(), path)\n",
    "        print(f\"Model saved to {path}\")\n",
    "\n",
    "    def load_model(self, path):\n",
    "        \"\"\"\n",
    "        Load a trained model from a file.\n",
    "\n",
    "        Args:\n",
    "            path (str): Path to the saved model file\n",
    "        \"\"\"\n",
    "\n",
    "        self.model.load_state_dict(torch.load(path, map_location=self.device))\n",
    "        print(f\"Model loaded from {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e139eac",
   "metadata": {},
   "source": [
    "## Execution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2594042",
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = PlagiarismDetection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd77b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader = detector.load_data(\n",
    "    csv_path=\"datasets/sdata.csv\", data_dir=\"datasets\"\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99d4c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "detector.train(train_loader, epochs=3)\n",
    "accuracy = detector.evaluate(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193779ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "detector.save_model(\"plagiarism_model_codeBERT.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
